@misc{gu2024mambalineartimesequencemodeling,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@inproceedings{escolano2024residual,
  title={Residual Dropout: A Simple Approach to Improve Transformer’s Data Efficiency},
  author={Escolano, Carlos and De Luca Fornaciari, Francesca and Melero, Maite},
  booktitle={The 3rd Annual Meeting of the ELRA-ISCA Special Interest Group on Under-resourced Languages@ LREC-COLING-2024 (SIGUL 2024): Turin, Italy},
  pages={294--299},
  year={2024},
  organization={ELRA Language Resources Association and the International Committee on~…}
}

@inproceedings{koehn2005europarl,
  title={Europarl: A parallel corpus for statistical machine translation},
  author={Koehn, Philipp},
  booktitle={Proceedings of machine translation summit x: papers},
  pages={79--86},
  year={2005}
}

@misc{pitorro2024effectivestatespacemodels,
      title={How Effective are State Space Models for Machine Translation?}, 
      author={Hugo Pitorro and Pavlo Vasylenko and Marcos Treviso and André F. T. Martins},
      year={2024},
      eprint={2407.05489},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.05489}, 
}

@article{lezmi2023time,
  title={Time series forecasting with transformer models and application to asset management},
  author={Lezmi, Edmond and Xu, Jiali},
  journal={Available at SSRN 4375798},
  year={2023}
}

@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={9},
  pages={11121--11128},
  year={2023}
}

@misc{huggingfaceProbabilisticTime,
	author = {},
	title = {{P}robabilistic {T}ime {S}eries {F}orecasting with {T}ransformers --- huggingface.co},
	howpublished = {\url{https://huggingface.co/blog/time-series-transformers}},
	year = {},
	note = {[Accessed 25-11-2024]},
}

@article{makridakis2024m4,
  title={The M4 Competition: 100,000 time series and 61 forecasting methods},
  author={Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={54--74},
  year={2020},
  publisher={Elsevier}
}

@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{yue2024medmamba,
  title={Medmamba: Vision mamba for medical image classification},
  author={Yue, Yubiao and Li, Zhenzhang},
  journal={arXiv preprint arXiv:2403.03849},
  year={2024}
}

@article{alex2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Alex, Krizhevsky},
  journal={https://www. cs. toronto. edu/kriz/learning-features-2009-TR. pdf},
  year={2009}
}

@inproceedings{wei2021petri,
  title={A petri dish for histopathology image analysis},
  author={Wei, Jerry and Suriawinata, Arief and Ren, Bing and Liu, Xiaoying and Lisovsky, Mikhail and Vaickus, Louis and Brown, Charles and Baker, Michael and Tomita, Naofumi and Torresani, Lorenzo and others},
  booktitle={Artificial Intelligence in Medicine: 19th International Conference on Artificial Intelligence in Medicine, AIME 2021, Virtual Event, June 15--18, 2021, Proceedings},
  pages={11--24},
  year={2021},
  organization={Springer}
}

@misc{paperswithcodePapersWithCifar10,
	author = {},
	title = {{P}apers with {C}ode - {C}{I}{F}{A}{R}-10 {B}enchmark ({I}mage {C}lassification) --- paperswithcode.com},
	howpublished = {\url{https://paperswithcode.com/sota/image-classification-on-cifar-10}},
	year = {},
	note = {[Accessed 28-11-2024]},
}

@misc{paperswithcodePapersWithCifar100,
	author = {},
	title = {{P}apers with {C}ode - {C}{I}{F}{A}{R}-100 {B}enchmark ({I}mage {C}lassification) --- paperswithcode.com},
	howpublished = {\url{https://paperswithcode.com/sota/image-classification-on-cifar-100}},
	year = {},
	note = {[Accessed 28-11-2024]},
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@misc{paperswithcodePapersWithMHIST,
	author = {},
	title = {{P}apers with {C}ode - {M}{H}{I}{S}{T} {B}enchmark ({C}lassification) --- paperswithcode.com},
	howpublished = {\url{https://paperswithcode.com/sota/classification-on-mhist}},
	year = {},
	note = {[Accessed 28-11-2024]},
}
