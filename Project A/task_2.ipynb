{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from ptflops import get_model_complexity_info\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PAD import PAD\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def train(net, trainloader, criterion, optimizer,device=device, epochs=20):\n",
    "    net.train()\n",
    "    net.to(device)\n",
    "    #create args.device \n",
    "    args = Args()\n",
    "    args.device = device\n",
    "    progress_bar = tqdm(range(epochs), desc=f'Training - No data available', leave=True)\n",
    "\n",
    "    for i in progress_bar:\n",
    "        loss_avg, acc_avg, num_exp = 0, 0, 0\n",
    "        for i_batch, datum in enumerate(trainloader):\n",
    "            img = datum[0].float().to(args.device)\n",
    "            lab = datum[1].long().to(args.device)\n",
    "            n_b = lab.shape[0]\n",
    "            output = net(img)[1]\n",
    "            loss = criterion(output, lab)\n",
    "            acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
    "            loss_avg += loss.item()*n_b\n",
    "            acc_avg += acc\n",
    "            num_exp += n_b\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_avg /= num_exp\n",
    "        acc_avg /= num_exp\n",
    "        progress_bar.set_description(f'Training - Loss: {loss_avg:.4f} - Accuracy: {acc_avg:.4f}')\n",
    "    return net\n",
    "\n",
    "def test(net, testloader, criterion, optimizer,device=device, return_acc=False):\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    args = Args()\n",
    "    args.device = device\n",
    "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
    "    for i_batch, datum in enumerate(testloader):\n",
    "        img = datum[0].float().to(args.device)\n",
    "        lab = datum[1].long().to(args.device)\n",
    "        n_b = lab.shape[0]\n",
    "\n",
    "        output = net(img)[1]\n",
    "        loss = criterion(output, lab)\n",
    "        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
    "        loss_avg += loss.item()*n_b\n",
    "        acc_avg += acc\n",
    "        num_exp += n_b\n",
    "\n",
    "\n",
    "    loss_avg /= num_exp\n",
    "    acc_avg /= num_exp\n",
    "    \n",
    "    if return_acc:\n",
    "        return acc_avg\n",
    "    else:\n",
    "        print(f'Accuracy of the network on the test images: {acc_avg*100}%')\n",
    "\n",
    "def count_flops(net, channel, im_size):\n",
    "    param = (channel, im_size[0], im_size[1])\n",
    "    flops, params = get_model_complexity_info(net, param, as_strings=True, print_per_layer_stat=False)\n",
    "    print(f'FLOPs: {flops}')\n",
    "    print(f'Params: {params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST Dataset\n",
    "data_path = \"./Project A/PAD\"\n",
    "\n",
    "#create a folder to save the synthetic dataset\n",
    "if not os.path.exists(os.path.join(data_path, 'synthetic_dataset_MNIST_gaussian')):\n",
    "    os.makedirs(os.path.join(data_path, 'synthetic_dataset_MNIST_gaussian'))\n",
    "\n",
    "#load the dataset\n",
    "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(\"MNIST\", data_path)\n",
    "\n",
    "#define the model\n",
    "net = 'ConvNetD3'\n",
    "\n",
    "Distillator = PAD(device, net, channel, num_classes, im_size, dst_train, 'synthetic_dataset_MNIST_gaussian', batch_size = 64, M=2,\n",
    "                 K=100, T=10, eta_S=0.1, zeta_S=1, eta_theta=0.01, zeta_theta=50, alpha=0.01, minibatches_size=64)\n",
    "\n",
    "mean_gaussian = 0\n",
    "std_gaussian = 1\n",
    "#Distillator.initialize_synthetic_dataset_from_gaussian_noise(mean_gaussian,std_gaussian)\n",
    "#Distillator.initialize_synthetic_dataset_from_real()\n",
    "\n",
    "condensed_dataset = Distillator.train(\"Gaussian\", mean_gaussian, std_gaussian)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
