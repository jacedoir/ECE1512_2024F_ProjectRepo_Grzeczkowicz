\documentclass[onecolumn]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{Tabbing}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[hidelinks]{hyperref}
\usepackage{chngpage}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{soul,color}
\usepackage{float}
\usepackage{graphicx,float,wrapfig}
% code listing settings
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    aboveskip={1.0\baselineskip},
    belowskip={1.0\baselineskip},
    columns=fixed,
    extendedchars=true,
    breaklines=true,
    tabsize=4,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=lines,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color[rgb]{0.627,0.126,0.941},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{01,0,0},
    numbers=left,
    numberstyle=\small,
    stepnumber=1,
    numbersep=10pt,
    captionpos=t,
    escapeinside={\%*}{*)}
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Add page numbers
\fancyhf{} % Clear all headers and footers
\fancyfoot[C]{\thepage} % Footer centered with page number
\renewcommand{\headrulewidth}{0pt} % Remove header rule
\renewcommand{\footrulewidth}{0pt} % Remove footer rule

\pagestyle{fancy} % Enable fancy header/footer

\begin{document}

\title{ECE1512 Project A Report}

\author{\IEEEauthorblockN{Rémi Grzeczkowicz} \\
\IEEEauthorblockA{\textit{MScAC Student} \\
\textit{University of Toronto - Department of Computer Science}\\
Student Number: 1010905399\\
remigrz@cs.toronto.edu}
}

\maketitle
\thispagestyle{fancy} % Apply fancy header/footer, including the page number on the first page


% \begin{abstract}
% This document is a model and instructions for \LaTeX.
% This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
% or Math in Paper Title or Abstract.
% \end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

%Table des matières centrée sur une nouvelle page
\vfill
\tableofcontents
\vfill

\newpage

\section{Introduction}

\section{Task 1 - DataDAM}
This part relies on the paper \cite{sajedi2023datadam}.

\subsection{Part 1 - Questions anwsering}
\begin{enumerate}[label=(\alph*)]
    \item In this paper, the purpose of using Dataset distillation is to reduce the training cost while preserving the performance of the model.
    \vspace{3mm}
    \item The advantages of their methodology over state-of-the-art are:
    \begin{itemize}
        \item It achieved unbiased representation of the real data distribution.
        \item It does not rely on rely on pre-trained network parameters or employ bi-level optimization.
        \item It has a reduced memory cost with a lower run time thanks to the fact that DataDAM does not use an inner-loop bi-level optimization.
        \item It outperformed other distillation methods except for one case where Matching Training Trajectory (MTT) performed better on CIFAR-10 with 10 Impage Per Class (IPC). MIT got an accuracy of $56.5\% \pm 0.7$ while DataDAM got $54.2\% \pm 0.8$.
    \end{itemize}
    \vspace{3mm}
    \item The novelty provided by this paper is the use of attention in data distillation. Indeed it has been used in knowledge distillation but never in dataset distillation.
    \vspace{3mm}
    \item The methodology is as follows:
    \begin{enumerate}[label=(\arabic*)]
        \item Initialize a synthetic dataset $\mathcal{S}$ either using random noise or sampling from the original training dataset $\mathcal{T}$.
        \item For each class $k$ a batch $B_T^k$ of real images and a batch $B_S^k$ of synthetic images are sampled from $\mathcal{T}$ and $\mathcal{S}$ respectively.
        \item Then a neural network $\phi_\theta$ is employed to extract features from the images. The network have different layers, each creating a feature map. This multiple feature maps allow to capture low-level, mid-level and high-level representations of the data.
        \item Using the feature maps of each layer, the Spatial Attention Matching (SAM) module generates an attention map for real and synthetic images. The attention map is formulated as $A(f_{\theta,l}^{T_k}) = \sum_{i=1}^{C_l} | (f_{\theta,l}^{T_k})_i|^p$ where $(f_{\theta,l}^{T_k})_i$ is the $i$-th feature map in the $l$th layer, $C_l$ is the number of channels and $p$ is a parameter to adjust the weights of the feature maps.
        \item The attention maps for both datasets are then compared using the loss function $\mathcal{L}_{SAM}$.
        \item The output of the network for each dataset is also compared using the loss function $\mathcal{L}_{MMD}$ based on the Maximum Mean Discrepancy (MMD).
        \item The total loss is then given by $\mathcal{L} = \mathcal{L}_{SAM} + \mathcal{L}_{MMD}$.
        \item Then $\mathcal{S}$ is updated such as $\mathcal{S} = arg \min_{\mathcal{S}} \mathcal{L}$.
    \end{enumerate}
    \vspace{3mm}
    \item DataDAM could be used in machine learning for continual learning by providing an efficient memory management method by storing the synthetic data in the memory instead of the real data. This allows for a better memory usage and a lower computational cost. DataDAM could also be used for neural architecture search. Indeed, instead of training many architectures on the full dataset, those architectures could trained on the distilled dataset, leading to a faster search.
\end{enumerate}

\subsection{Part 2 - Data Distillation Learning using DataDAM}
\subsubsection{Build the distillation model}
To build the distillation model, we used the code provided by the author of \cite{sajedi2023datadam} in the repo \cite{githubGitHubDataDistillationDataDAM}. This allowed us to create a DataDAM class that we could use. Some change were required to make the understanding easier and to fit our framework as well as remove the unneeded parts.
\\
In \cite{githubGitHubDataDistillationDataDAM}, we can notice that the authors introduced a factor $100$ in their loss. After various attempt, and because they use more iterations than we do, we decided to set the factor to $10 000$ so that the loss is not too small and the dataset is updated correctly.

\subsubsection{Distillation of MNIST dataset from real data}
To generate the synthetic dataset associated with the MNIST dataset, we used the DataDAM class. The parameters used were : $model = ConvNet3D$, $IPC = 10$, $K=100$, $T=10$, $\eta_S = 0.1$, $\zeta_S = 1$, $\eta_\theta = 0.01$, $\zeta_\theta=50$, $\lambda_{mmd}=0.01$ and $minibatches_{size}=256$.
\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/MNIST_datadam_before_distil_real.png}
        \caption{The synthetic dataset before distillation from real data}
        \label{fig:MNIST_datadam_before_distil_real}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/MNIST_datadam_after_distil_real.png}
        \caption{The synthetic dataset after distillation from real data}
        \label{fig:MNIST_datadam_after_distil_real}
    \end{subfigure}
    \caption{The synthetic dataset before and after distillation from real data}
\end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/MNIST_datadam_after_distil_real.png}
%     \caption{The synthetic dataset after distillation from real data}
%     \label{fig:MNIST_datadam_after_distil_real}
% \end{figure}

Figure \ref{fig:MNIST_datadam_before_distil_real} shows the synthetic dataset before distillation from real data and Figure \ref{fig:MNIST_datadam_after_distil_real} shows the synthetic dataset after distillation from real data. At first glance, we can see that the synthetic dataset is almost identical to the real dataset. To have a better understanding of the difference between the two datasets, we can look at Figure \ref{fig:MNIST_diff_real}. This figure shows the difference between the synthetic dataset before and after distillation from real data. We can see that the difference then resides in the detail of the shape of the digits. This is a good sign as it means that the synthetic dataset condenses the information of the real dataset.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/MNIST_diff_real.png}
    \caption{The difference between the synthetic dataset before and after distillation from real data}
    \label{fig:MNIST_diff_real}
\end{figure}

\subsubsection{Distillation of MNIST dataset from gaussian noise}
To generate the gaussin noise, we used a standard normal distribution. The parameters used were the same as for the distillation from real data.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/MNIST_datadam_before_distil_gaussian.png}
        \caption{The synthetic dataset before distillation from gaussian noise}
        \label{fig:MNIST_datadam_before_distil_gaussian}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/MNIST_datadam_after_distil_gaussian.png}
        \caption{The synthetic dataset after distillation from gaussian noise}
        \label{fig:MNIST_datadam_after_distil_gaussian}
    \end{subfigure}
    \caption{The synthetic dataset before and after distillation from gaussian noise}
\end{figure}

Figure \ref{fig:MNIST_datadam_before_distil_gaussian} shows the synthetic dataset before distillation from gaussian noise and Figure \ref{fig:MNIST_datadam_after_distil_gaussian} shows the synthetic dataset after distillation from gaussian noise. We can see that the distillation method succeeded in creating a synthetic dataset that shows digits we can recognize. However, those digits are not high quality.
\\
\subsubsection{Resutls and discussion for the MNIST dataset}
We then trained a ConvNet3D model for 50 epochs on the full MNIST dataset and on the two synthetic datasets. The results are shown in Table \ref{tab:MNIST_results}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Dataset & Full & From gaussian & From real \\
        \hline
        Accuracy & $99.49\%$ & $74.44\%$ & $91.81\%$ \\
        \hline
        Training Time & 7 min 7 s & $<$ 1 s & $<$ 1 s \\
        \hline
    \end{tabular}
    \caption{Results for the MNIST dataset}
    \label{tab:MNIST_results}
\end{table}

We can see that the model trained on the real dataset as the best accuracy. However, we also see that training was much longer than for the synthetic datasets. The model trained on the synthetic dataset from gaussian noise has the worst accuracy. This is expected as the synthetic dataset is of low quality. The model trained on the synthetic dataset from real data has a good accuracy and a training time that is much lower than the model trained on the full dataset. This shows that the distillation method is efficient in reducing the training time while preserving the performance of the model.
\\
We then evaluate the dataset on a cross-architecture setup. For this, we train AlexNet on the full MNIST dataset and on the two synthetic datasets. The results are shown in Table \ref{tab:MNIST_results_cross_architecture}. Results show that the cross architecture permforms poorly on the synthetic datasets. This could be due by the fact that the synthetic dataset is created using the specificity of a ConvNet3D model and that AlexNet is not able to capture the same information.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Dataset & Full & From gaussian & From real \\
        \hline
        Accuracy & $99.19\%$ & $17.9\%$ & $23.48\%$ \\
        \hline
        Training Time & 9 min 22 s & $<$ 1 s & $<$ 1 s \\
        \hline
    \end{tabular}
    \caption{Results for the MNIST dataset on cross-architecture setup (AlexNet)}
    \label{tab:MNIST_results_cross_architecture}
\end{table}

\subsubsection{Distillation of the MHIST dataset from real data}
To generate the synthetic dataset associated with the MHIST dataset, we used the DataDAM class. The parameters used were : $model = ConvNet7D$, $IPC = 50$, $K=200$, $T=10$, $\eta_S = 100$, $\zeta_S = 1$, $\eta_\theta = 0.01$, $\zeta_\theta=50$, $\lambda_{mmd}=0.01$ and $minibatches_{size}=128$.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/MHIST_datadam_before_distil_real.png}
        \caption{The synthetic dataset before distillation from real data}
        \label{fig:MHIST_datadam_before_distil_real}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/MHIST_datadam_after_distil_real.png}
        \caption{The synthetic dataset after distillation from real data}
        \label{fig:MHIST_datadam_after_distil_real}
    \end{subfigure}
    \caption{The synthetic dataset before and after distillation from real data}
\end{figure}

Figure \ref{fig:MHIST_datadam_before_distil_real} shows the synthetic dataset before distillation from real data and Figure \ref{fig:MHIST_datadam_after_distil_real} shows the synthetic dataset after distillation from real data. At first glance, we can see that the synthetic dataset is almost identical to the real dataset. The main difference could be observed in the background of the images. To have a better understanding of the difference between the two datasets, we can look at Figure \ref{fig:MHIST_diff_real}. This figure shows the difference between the synthetic dataset before and after distillation from real data. We can then see that the entire image is updated, meaning that the synthetic dataset is not a simple copy of the real dataset.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/MHIST_diff_real.png}
    \caption{The difference between the synthetic dataset before and after distillation from real data}
    \label{fig:MHIST_diff_real}
\end{figure}

\subsubsection{Distillation of the MHIST dataset from gaussian noise}
To generate the gaussin noise, we used a standard normal distribution. The parameters used were the same as for the distillation from real data.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/MHIST_datadam_before_distil_gaussian.png}
        \caption{The synthetic dataset before distillation from gaussian noise}
        \label{fig:MHIST_datadam_before_distil_gaussian}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/MHIST_datadam_after_distil_gaussian.png}
        \caption{The synthetic dataset after distillation from gaussian noise}
        \label{fig:MHIST_datadam_after_distil_gaussian}
    \end{subfigure}
    \caption{The synthetic dataset before and after distillation from gaussian noise}
\end{figure}

Figure \ref{fig:MHIST_datadam_before_distil_gaussian} shows the synthetic dataset before distillation from gaussian noise and Figure \ref{fig:MHIST_datadam_after_distil_gaussian} shows the synthetic dataset after distillation from gaussian noise. On the contrary of the MNIST dataset, the synthetic dataset from gaussian noise of the MHIST dataset is not recognizable. The results was expected as the data is much more complex with the three channels as well as in terms of the content of each image.
\\
\subsubsection{Resutls and discussion for the MHIST dataset}
\label{sec:MHIST_results}
We then trained a ConvNet7D model for 50 epochs on the full MHIST dataset and on the two synthetic datasets. The results are shown in Table \ref{tab:MHIST_results}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Dataset & Full & From gaussian & From real \\
        \hline
        Accuracy & $80.14\%$ & $53.63\%$ & $63.87\%$ \\
        \hline
        Training Time & 6 min 12 s & 10 s & 10 s \\
        \hline
    \end{tabular}
    \caption{Results for the MHIST dataset}
    \label{tab:MHIST_results}
\end{table}

We can see that the model trained on the real dataset as the best accuracy. However, we also see that training was much longer than for the synthetic datasets. The model trained from the synthetic dataset from gaussian noise has the worst accuracy, close to the random guess. The model trained on the synthetic dataset from real data has a better accuracy. The training time is much lower than the model trained on the full dataset. This shows that the distillation method is efficient in reducing the training time but comes with a loss of accuracy. This is expected as the synthetic dataset is not a perfect representation of the real dataset.
We then evaluate the dataset on a cross-architecture setup. For this, we train AlexNet on the full MHIST dataset and on the two synthetic datasets. The results are shown in Table \ref{tab:MHIST_results_cross_architecture}. Surprinsingly, the model trained on the synthetic dataset from real data has a lower accuracy than the model trained on the synthetic dataset from gaussian noise and the latter performs nearly as well as the model trained on the full dataset. This could be due to the fact that the synthetic dataset from real data is too good for the ConvNet7D model where as the synthetic dataset from gaussian noise is not good enough so it generalizes better on a different architecture.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Dataset & Full & From gaussian & From real \\
        \hline
        Accuracy & $68.37\%$ & $63.15\%$ & $36.87\%$ \\
        \hline
        Training Time & 6 min 40 s & 17 s & 18 s \\
        \hline
    \end{tabular}
    \caption{Results for the MHIST dataset on cross-architecture setup (AlexNet)}
    \label{tab:MHIST_results_cross_architecture}
\end{table}

\subsubsection{Neural architecture search}
We then proposed to use the synthetic dataset to perform a neural architecture search. We want to find the best ConvNet model to classify either the MNIST or the MHIST dataset. We will search the model among ConvNetD1, ConvNetD2, ConvNetD3, ConvNetD4, ConvNetD5, ConvNetW32, ConvNetW64, ConvNetW128', ConvNetW256, ConvNetAS, ConvNetAR, ConvNetAL, ConvNetASwish, ConvNetASwishBN, ConvNetNN, ConvNetBN, ConvNetLN, ConvNetIN, ConvNetGN, ConvNetNP, ConvNetMP and  ConvNetAP.
\\
Results for the MNIST dataset are shown in Table \ref{tab:NAS_results_MNIST}. We can observe that searching the best architecture using the synthetic dataset is much faster than just training the best architecture on the full dataset. In both cases, the best model achieve a high accuracy on the full dataset. This shows that the synthetic dataset is efficient to perform a neural architecture search.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Search & From Real & From Gaussian \\
        \hline
        Best Model & ConvNetW256 & ConvNetW64\\
        \hline
        Accuracy on best model trained with synthetic data & $91.92\%$ & $75.79\%$ \\
        \hline
        Research time & 34 s & 82.83 s \\
        \hline
        Accuracy on best model trained on full data & $99.52\%$ & $99.5\%$ \\
        \hline
        Training time on full data & 8 min 4 s & 7 min 27 s \\
        \hline
    \end{tabular}
    \caption{Results for the MNIST dataset}
    \label{tab:NAS_results_MNIST}
\end{table}

For the MHIST dataset, the results are shown in Table \ref{tab:NAS_results_MHIST}. We can observe that searching the best architecture using the synthetic dataset is much faster than just training the best architecture on the full dataset. However, the model found using the neural architecture search is not as good as the model trained on the full dataset in part \ref{sec:MHIST_results}. This could be due to the fact that, contrary to the MNIST dataset, the synthetic dataset of the MHIST dataset is not perfroming really well and then using it for a neural architecture search is not as efficient as it propagates the error of the synthetic dataset.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Search & From Real & From Gaussian \\
        \hline
        Best Model & ConvNetGN & ConvNetAS\\
        \hline
        Accuracy on best model trained with synthetic data & $64.79\%$ & $63.15\%$ \\
        \hline
        Research time & 10 min 56 s & 10 min 55 s \\
        \hline
        Accuracy on best model trained on full data & $67.04\%$ & $63.66\%$ \\
        \hline
        Training time on full data & 3 min 45 s & 3 min 36 s \\
        \hline
    \end{tabular}
    \caption{Results for the MHIST dataset}
    \label{tab:NAS_results_MHIST}
\end{table}

\section{Task 2 - PAD}
This part relies on the paper \cite{li2024prioritize}.
\subsection{Part 1 - Questions anwsering}
\begin{enumerate}[label=(\alph*)]
    \item The PAD method wants to solve the problem of misaligned information whether extracted or embedded into the synthetic dataset. The goal is to une only high-quality informations to improve the synthetic dataset.
    \vspace{3mm}
    \item PAD introduced two novelties. The first one is the filtering of misaligned information during the extraction step by chosing the data to use according to the difficulty of samples and the image per class ration. The second novelty is the use of a layer-wise filtering in embedding. PAD only uses the deep layers of the agent model to perform distillation as they tend to captures higher quality information.
    \vspace{3mm}
    \item The methodology is as follows:
    \begin{enumerate}[label=(\arabic*)]
        \item The real dataset is scored using a difficulty scoring function $\chi_t(x, y) = \mathbb{E} \left\| p(w_t, x) - y \right\|_2$ where $x$ is a data point, $y$ its label and $p(w_t,x)=\sigma(f(w_t, x))$ is the output of a model $f$ transformed into a probability distribution.
        \item Using this difficulty score, a data scheduler is define as follow. All the easy data are selected first, then the hard data is gradually added to the set following the order of difficulty. When all the data is in the set, the easy data is gradually removed.
        \item An agent model is trained on the real dataset $\mathcal{D}_R$ using a Trajectory-Matching based method and the scheduler described. Then the change of parameters are stored. Let $\{\theta_t^*\}_0^N$ be a parameter sequence of the agent model (it is also called the trajectory). 
        \item At each ineration, $\theta_t^*$ and $\theta_{t+M}^*$ are selected as start and target parameters. Let $\hat\theta_t$ be the parameters of the student agent model trained on the synthetic dataset $\mathcal{D}_S$. Then for $N$ steps $\hat\theta_{t+i+1} = \hat\theta_{t+i} + \alpha \nabla\mathcal{L}_{CE}(\hat\theta_{t+i}, \mathcal{D}_S)$ where $\mathcal{L}_{CE}$ is the cross-entropy loss.
        \item To filter information $\hat\theta_{t+N}$ is defined such as only the $L-k$ last layers are used for matching where $k=\alpha*L$ and $\alpha$ is a hyperparameter that should be low in case of small IPC and high in case of high IPC.
        \item Then the synthetic dataset is updated such as it minimizes $\mathcal{L}$ where $\mathcal{L}=\frac{\left\|\hat\theta_{t+N}-\theta^*_{t+M}\right\|}{\left\|\theta^*_{t+M}-\theta^*_t\right\|}$.
    \end{enumerate}
    \vspace{3mm}
    \item The advantages of PAD are: an improved alignment of information, a high-level of information (by discarding shallow layers) and, according to the paper, a better performance than other methods. The desadvanteges are the introduction of hyperparameter (notably $\alpha$) that have to be tuned, the dependence of the difficulty scoring function that could be hard to define/improve. I believe that this method could distill the dataset correctly accordingt to the performances authors got. I believe that PAD will have more trouble with large scale dataset as the difficulty scoring function may score the entire dataset as difficult and thus leading to a non optimal scheduler.
\end{enumerate}

\subsection{Part 2 - Data Distillation Learning using PAD}
\subsubsection{Build the distillation model}
To build the distillation model, we used the code provided by the author of \cite{li2024prioritize} in the repo \cite{githubGitHubNUSHPCAILabPAD}. This allowed us to create a PAD class that we could use. As previously, some change were required to make the understanding easier and to fit our framework as well as remove the unneeded parts.

\bibliographystyle{plain}
\bibliography{mybibliography}


\end{document}
