\documentclass[onecolumn]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{Tabbing}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[hidelinks]{hyperref}
\usepackage{chngpage}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{soul,color}
\usepackage{float}
\usepackage{graphicx,float,wrapfig}
% code listing settings
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    aboveskip={1.0\baselineskip},
    belowskip={1.0\baselineskip},
    columns=fixed,
    extendedchars=true,
    breaklines=true,
    tabsize=4,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=lines,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color[rgb]{0.627,0.126,0.941},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{01,0,0},
    numbers=left,
    numberstyle=\small,
    stepnumber=1,
    numbersep=10pt,
    captionpos=t,
    escapeinside={\%*}{*)}
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Add page numbers
\fancyhf{} % Clear all headers and footers
\fancyfoot[C]{\thepage} % Footer centered with page number
\renewcommand{\headrulewidth}{0pt} % Remove header rule
\renewcommand{\footrulewidth}{0pt} % Remove footer rule

\pagestyle{fancy} % Enable fancy header/footer

\begin{document}

\title{ECE1512 Project A Report}

\author{\IEEEauthorblockN{RÃ©mi Grzeczkowicz}
\IEEEauthorblockA{\textit{MScAC Student} \\
\textit{University of Toronto}\\
Student Number: 1010905399\\
remigrz@cs.toronto.edu}
}

\maketitle
\thispagestyle{fancy} % Apply fancy header/footer, including the page number on the first page


% \begin{abstract}
% This document is a model and instructions for \LaTeX.
% This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
% or Math in Paper Title or Abstract.
% \end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\section{Introduction}

\section{Task 1}
This part relies on the paper \cite{sajedi2023datadam}.

\subsection{Part 1}
\begin{enumerate}[label=(\alph*)]
    \item In this paper, the purpose of using Dataset distillation is to reduce the training cost while preserving the performance of the model.
    \vspace{3mm}
    \item The advantages of their methodology over state-of-the-art are:
    \begin{itemize}
        \item It achieved unbiased representation of the real data distribution.
        \item It does not rely on rely on pre-trained network parameters or employ bi-level optimization.
        \item It has a reduced memory cost with a lower run time thanks to the fact that DataDAM does not use an inner-loop bi-level optimization.
        \item It outperformed other distillation methods except for one case where Matching Training Trajectory (MTT) performed better on CIFAR-10 with 10 Impage Per Class (IPC). MIT got an accuracy of $56.5\% \pm 0.7$ while DataDAM got $54.2\% \pm 0.8$.
    \end{itemize}
    \vspace{3mm}
    \item The novelty provided by this paper is the use of attention in data distillation. Indeed it has been used in knowledge distillation but never in dataset distillation.
    \vspace{3mm}
    \item The methodology is as follows:
    \begin{enumerate}[label=(\arabic*)]
        \item Initialize a synthetic dataset $\mathcal{S}$ either using random noise or sampling from the original training dataset $\mathcal{T}$.
        \item For each class $k$ a batch $B_T^k$ of real images and a batch $B_S^k$ of synthetic images are sampled from $\mathcal{T}$ and $\mathcal{S}$ respectively.
        \item Then a neural network $\phi_\theta$ is employed to extract features from the images. The network have different layers, each creating a feature map. This multiple feature maps allow to capture low-level, mid-level and high-level representations of the data.
        \item Using the feature maps of each layer, the Spatial Attention Matching (SAM) module generates an attention map for real and synthetic images. The attention map is formulated as $A(f_{\theta,l}^{T_k}) = \sum_{i=1}^{C_l} | (f_{\theta,l}^{T_k})_i|^p$ where $(f_{\theta,l}^{T_k})_i$ is the $i$-th feature map in the $l$th layer, $C_l$ is the number of channels and $p$ is a parameter to adjust the weights of the feature maps.
        \item The attention maps for both datasets are then compared using the loss function $\mathcal{L}_{SAM}$.
        \item The output of the network for each dataset is also compared using the loss function $\mathcal{L}_{MMD}$ based on the Maximum Mean Discrepancy (MMD).
        \item The total loss is then given by $\mathcal{L} = \mathcal{L}_{SAM} + \mathcal{L}_{MMD}$.
        \item Then $\mathcal{S}$ is updated such as $\mathcal{S} = arg \min_{\mathcal{S}} \mathcal{L}$.
    \end{enumerate}
    \vspace{3mm}
    \item DataDAM could be used in machine learning for continual learning by providing an efficient memory management method by storing the synthetic data in the memory instead of the real data. This allows for a better memory usage and a lower computational cost. DataDAM could also be used for neural architecture search. Indeed, instead of training many architectures on the full dataset, those architectures could trained on the distilled dataset, leading to a faster search.
\end{enumerate}

\subsection{Part 2}

\section{Task 2}
This part relies on the paper \cite{li2024prioritize}.
\subsection{Part 1}
\begin{enumerate}[label=(\alph*)]
    \item The PAD method wants to solve the problem of misaligned information whether extracted or embedded into the synthetic dataset. The goal is to une only high-quality informations to improve the synthetic dataset.
    \vspace{3mm}
    \item PAD introduced two novelties. The first one is the filtering of misaligned information during the extraction step by chosing the data to use according to the difficulty of samples and the image per class ration. The second novelty is the use of a layer-wise filtering in embedding. PAD only uses the deep layers of the agent model to perform distillation as they tend to captures higher quality information.
    \vspace{3mm}
    \item The methodology is as follows:
    \begin{enumerate}[label=(\arabic*)]
        \item The real dataset is scored using a difficulty scoring function $\chi_t(x, y) = \mathbb{E} \left\| p(w_t, x) - y \right\|_2$ where $x$ is a data point, $y$ its label and $p(w_t,x)=\sigma(f(w_t, x))$ is the output of a model $f$ transformed into a probability distribution.
        \item Using this difficulty score, a data scheduler is define as follow. All the easy data are selected first, then the hard data is gradually added to the set following the order of difficulty. When all the data is in the set, the easy data is gradually removed.
        \item An agent model is trained on the real dataset $\mathcal{D}_R$ using a Trajectory-Matching based method and the scheduler described. Then the change of parameters are stored. Let $\{\theta_t^*\}_0^N$ be a parameter sequence of the agent model (it is also called the trajectory). 
        \item At each ineration, $\theta_t^*$ and $\theta_{t+M}^*$ are selected as start and target parameters. Let $\hat\theta_t$ be the parameters of the student agent model trained on the synthetic dataset $\mathcal{D}_S$. Then for $N$ steps $\hat\theta_{t+i+1} = \hat\theta_{t+i} + \alpha \nabla\mathcal{L}_{CE}(\hat\theta_{t+i}, \mathcal{D}_S)$ where $\mathcal{L}_{CE}$ is the cross-entropy loss.
        \item To filter information $\hat\theta_{t+N}$ is defined such as only the $L-k$ last layers are used for matching where $k=\alpha*L$ and $\alpha$ is a hyperparameter that should be low in case of small IPC and high in case of high IPC.
        \item Then the synthetic dataset is updated such as it minimizes $\mathcal{L}$ where $\mathcal{L}=\frac{\left\|\hat\theta_{t+N}-\theta^*_{t+M}\right\|}{\left\|\theta^*_{t+M}-\theta^*_t\right\|}$.
    \end{enumerate}
    \vspace{3mm}
    \item The advantages of PAD are: an improved alignment of information, a high-level of information (by discarding shallow layers) and, according to the paper, a better performance than other methods. The desadvanteges are the introduction of hyperparameter (notably $\alpha$) that have to be tuned, the dependence of the difficulty scoring function that could be hard to define/improve. I believe that this method could distill the dataset correctly accordingt to the performances authors got. I believe that PAD will have more trouble with large scale dataset as the difficulty scoring function may score the entire dataset as difficult and thus leading to a non optimal scheduler.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{mybibliography}


\end{document}
